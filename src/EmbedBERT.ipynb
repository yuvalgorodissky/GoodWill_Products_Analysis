{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a50cd10-e350-44c1-9c45-0ebffce0fd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "from scipy.sparse import hstack\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from preprocessing import load_and_combine_csv_files,clean_and_label_data\n",
    "import joblib\n",
    "import os\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc9104a3-cc56-44f7-bb47-4388d3a25586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoodwillDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get text data\n",
    "        title = str(self.df.iloc[idx]['title'])\n",
    "        description = str(self.df.iloc[idx]['description'])\n",
    "        \n",
    "        # Tokenize title and description separately\n",
    "        title_encoding = self.tokenizer(\n",
    "            title,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        desc_encoding = self.tokenizer(\n",
    "            description,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get target price\n",
    "        price = float(self.df.iloc[idx]['currentPrice'])\n",
    "        \n",
    "        return {\n",
    "            'title_input_ids': title_encoding['input_ids'].squeeze(),\n",
    "            'title_attention_mask': title_encoding['attention_mask'].squeeze(),\n",
    "            'desc_input_ids': desc_encoding['input_ids'].squeeze(),\n",
    "            'desc_attention_mask': desc_encoding['attention_mask'].squeeze(),\n",
    "            'price': torch.tensor(price, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a08aba9f-66cf-4cbe-a2c8-ec36dcf11bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PricePredictor(nn.Module):\n",
    "    def __init__(self, transformer_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.transformer = transformer_model\n",
    "        self.transformer_dim = self.transformer.config.hidden_size\n",
    "        \n",
    "        # Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.transformer_dim * 2, self.transformer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.transformer_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        # ReLU for final output to ensure positive prices\n",
    "        self.final_activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, title_input_ids, title_attention_mask, \n",
    "                desc_input_ids, desc_attention_mask):\n",
    "        # Get transformer outputs for title and description\n",
    "        title_output = self.transformer(\n",
    "            input_ids=title_input_ids,\n",
    "            attention_mask=title_attention_mask\n",
    "        )\n",
    "        \n",
    "        desc_output = self.transformer(\n",
    "            input_ids=desc_input_ids,\n",
    "            attention_mask=desc_attention_mask\n",
    "        )\n",
    "        \n",
    "        # Get [CLS] token outputs\n",
    "        title_features = title_output.last_hidden_state[:, 0, :]\n",
    "        desc_features = desc_output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined = torch.cat([title_features, desc_features], dim=1)\n",
    "        \n",
    "        # Fuse features\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        output = self.fc(fused)\n",
    "        \n",
    "        # Ensure positive output\n",
    "        return self.final_activation(output).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f31359d3-c9fe-4572-8eea-5dfcaa74e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters_by_layer(model):\n",
    "   print(\"\\nParameters by Layer:\")\n",
    "   print(\"-\" * 50)\n",
    "   \n",
    "   # BERT layers\n",
    "   bert_params = sum(p.numel() for p in model.transformer.parameters())\n",
    "   print(f\"1. BERT Encoder: {bert_params:,} parameters\")\n",
    "   \n",
    "   # Fusion layers \n",
    "   print(\"\\n2. Fusion Layer:\")\n",
    "   for i, layer in enumerate(model.fusion):\n",
    "       layer_params = sum(p.numel() for p in layer.parameters())\n",
    "       print(f\"   {i+1}. {layer.__class__.__name__}: {layer_params:,} parameters\")\n",
    "   \n",
    "   # Fully Connected layers\n",
    "   print(\"\\n3. Fully Connected Layers:\")\n",
    "   for i, layer in enumerate(model.fc):\n",
    "       layer_params = sum(p.numel() for p in layer.parameters())\n",
    "       print(f\"   {i+1}. {layer.__class__.__name__}: {layer_params:,} parameters\")\n",
    "   \n",
    "   # Total parameters\n",
    "   total_params = sum(p.numel() for p in model.parameters())\n",
    "   print(\"\\n\" + \"-\" * 50)\n",
    "   print(f\"Total Parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67f40ea9-7695-4bde-941a-133ed39d8875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sise/eliorsu-group/yuvalgor/courses/Data-mining-in-Big-Data/src/preprocessing.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['title'] = df['title'].apply(lambda x: str(x).lower().strip() if pd.notna(x) else '')\n",
      "/sise/eliorsu-group/yuvalgor/courses/Data-mining-in-Big-Data/src/preprocessing.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['description'] = df['description'].apply(lambda x: str(x).lower().strip() if pd.notna(x) else '')\n",
      "/sise/eliorsu-group/yuvalgor/courses/Data-mining-in-Big-Data/src/preprocessing.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['state_encoded'] = le_state.transform(df['pickupState'])\n",
      "/sise/eliorsu-group/yuvalgor/courses/Data-mining-in-Big-Data/src/preprocessing.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['category_encoded'] = le_category.transform(df['mainCategory'])\n"
     ]
    }
   ],
   "source": [
    "# Parameters for loading data\n",
    "directory = \"/sise/eliorsu-group/yuvalgor/courses/Data-mining-in-Big-Data/datasets/\"\n",
    "base_filename = \"goodwill_items_job_\"\n",
    "num_files = 30\n",
    "\n",
    "# Load and combine the CSV files\n",
    "combined_df = load_and_combine_csv_files(directory, base_filename, num_files)\n",
    "\n",
    "# Clean and label the data\n",
    "cleaned_df, le_state, le_category = clean_and_label_data(combined_df)\n",
    "\n",
    "# Split data using first 400000 rows for training\n",
    "train_val_df = cleaned_df.iloc[:400000].copy()\n",
    "test_df = cleaned_df.iloc[400000:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cebc2d77-0d7e-4555-aca7-f3d9acbc7f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvalgor/.conda/envs/goodwill_proj/lib/python3.9/site-packages/huggingface_hub-0.27.0-py3.8.egg/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters by Layer:\n",
      "--------------------------------------------------\n",
      "1. BERT Encoder: 109,482,240 parameters\n",
      "\n",
      "2. Fusion Layer:\n",
      "   1. Linear: 1,180,416 parameters\n",
      "   2. ReLU: 0 parameters\n",
      "   3. Dropout: 0 parameters\n",
      "\n",
      "3. Fully Connected Layers:\n",
      "   1. Linear: 393,728 parameters\n",
      "   2. ReLU: 0 parameters\n",
      "   3. Dropout: 0 parameters\n",
      "   4. Linear: 131,328 parameters\n",
      "   5. ReLU: 0 parameters\n",
      "   6. Dropout: 0 parameters\n",
      "   7. Linear: 257 parameters\n",
      "\n",
      "--------------------------------------------------\n",
      "Total Parameters: 111,187,969\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize tokenizer and transformer model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "transformer_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = GoodwillDataset(train_val_df, tokenizer)\n",
    "test_dataset = GoodwillDataset(test_df, tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "model = PricePredictor(transformer_model).to(device)\n",
    "count_parameters_by_layer(model)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d43109da-c819-472c-bb20-0da8b4e8ebb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 15645.8238: 100%|██████████| 12500/12500 [50:32<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 15277.4210:  97%|█████████▋| 12173/12500 [49:21<01:19,  4.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 24\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, price)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Update total loss and progress bar\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/goodwill_proj/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/goodwill_proj/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/goodwill_proj/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader)\n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        title_input_ids = batch['title_input_ids'].to(device)\n",
    "        title_attention_mask = batch['title_attention_mask'].to(device)\n",
    "        desc_input_ids = batch['desc_input_ids'].to(device)\n",
    "        desc_attention_mask = batch['desc_attention_mask'].to(device)\n",
    "        price = batch['price'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(title_input_ids, title_attention_mask,\n",
    "                      desc_input_ids, desc_attention_mask)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, price)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update total loss and progress bar\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (progress_bar.n + 1)  # Current average loss\n",
    "        progress_bar.set_description(f'Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "num_epochs = 3\n",
    "save_dir = '/sise/eliorsu-group/yuvalgor/courses/Data-mining-in-Big-Data/models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    avg_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab90740f-f1e0-486c-8383-5b11ce7e110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "   model.eval()\n",
    "   predictions = []\n",
    "   actuals = []\n",
    "   \n",
    "   with torch.no_grad():\n",
    "       for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "           # Move batch to device\n",
    "           title_input_ids = batch['title_input_ids'].to(device)\n",
    "           title_attention_mask = batch['title_attention_mask'].to(device)\n",
    "           desc_input_ids = batch['desc_input_ids'].to(device)\n",
    "           desc_attention_mask = batch['desc_attention_mask'].to(device)\n",
    "           price = batch['price']\n",
    "           \n",
    "           # Get predictions\n",
    "           output = model(title_input_ids, title_attention_mask,\n",
    "                        desc_input_ids, desc_attention_mask)\n",
    "           \n",
    "           predictions.extend(output.cpu().numpy())\n",
    "           actuals.extend(price.numpy())\n",
    "   \n",
    "   return np.array(predictions), np.array(actuals)\n",
    "\n",
    "# Create save directory if it doesn't exist\n",
    "save_dir = '/sise/eliorsu-group/yuvalgor/courses/Data-mining-in-Big-Data/models'\n",
    "if not os.path.exists(save_dir):\n",
    "   os.makedirs(save_dir)\n",
    "\n",
    "# Get predictions and calculate metrics\n",
    "predictions, actuals = evaluate(model, test_loader, device)\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(actuals, predictions)\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\") \n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "\n",
    "# Save model with metrics\n",
    "model_name = 'EmbedBERT'\n",
    "model_path = os.path.join(save_dir, f'{model_name}.pth')\n",
    "torch.save({\n",
    "  'model_state_dict': model.state_dict(),\n",
    "  'test_mse': mse,\n",
    "  'test_rmse': rmse,\n",
    "  'test_r2': r2,\n",
    "  'tokenizer': tokenizer,\n",
    "}, model_path)\n",
    "print(f'\\nModel saved at: {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a07091-c711-4a1a-8b8a-026a79e1bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "# Add predictions and calculate metrics\n",
    "test_df['predicted_price'] = predictions\n",
    "test_df['actual_price'] = test_df['currentPrice']\n",
    "test_df['price_difference'] = test_df['predicted_price'] - test_df['actual_price']\n",
    "test_df['price_difference_pct'] = ((test_df['actual_price'] - test_df['predicted_price']) / test_df['predicted_price']) * -100\n",
    "\n",
    "# Create analysis dataframe\n",
    "analysis_df = test_df[[\n",
    "   'title',\n",
    "   'actual_price',\n",
    "   'predicted_price', \n",
    "   'price_difference',\n",
    "   'price_difference_pct',\n",
    "   'mainCategory',\n",
    "   'description',\n",
    "   'pickupState',\n",
    "   'imageUrls',\n",
    "   'itemId'\n",
    "]].copy()\n",
    "\n",
    "# Round numeric columns\n",
    "numeric_cols = ['actual_price', 'predicted_price', 'price_difference', 'price_difference_pct']\n",
    "analysis_df[numeric_cols] = analysis_df[numeric_cols].round(2)\n",
    "\n",
    "# Sort by price difference\n",
    "analysis_df = analysis_df.sort_values('price_difference', ascending=False)\n",
    "\n",
    "# Save results\n",
    "results_dir = '/sise/eliorsu-group/yuvalgor/courses/Data-mining-in-Big-Data/results'\n",
    "if not os.path.exists(results_dir):\n",
    "   os.makedirs(results_dir)\n",
    "   \n",
    "filename = f\"{model_name}_analysis_{timestamp}.csv\"\n",
    "save_path = os.path.join(results_dir, filename)\n",
    "analysis_df.to_csv(save_path, index=False)\n",
    "print(f\"Analysis results saved at {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7be9a-15a5-4418-866b-370ab9b33936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (goodwill_proj)",
   "language": "python",
   "name": "goodwill_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
