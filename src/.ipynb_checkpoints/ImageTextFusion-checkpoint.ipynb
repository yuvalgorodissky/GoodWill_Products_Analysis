{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc90037c-917f-4a76-b359-ebf88aa01414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "from scipy.sparse import hstack\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from preprocessing import load_and_combine_csv_files,clean_and_label_data\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624f7238-764c-4a13-a706-8c5f74921667",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GoodwillDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Image preprocessing\n",
    "        self.image_transforms = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def load_image_from_url(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            return self.image_transforms(img)\n",
    "        except:\n",
    "            # Return a blank image if there's an error\n",
    "            return torch.zeros(3, 224, 224)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get text data\n",
    "        title = str(self.df.iloc[idx]['title'])\n",
    "        description = str(self.df.iloc[idx]['description'])\n",
    "        \n",
    "        # Get image\n",
    "        image_url = str(self.df.iloc[idx]['imageUrls'])\n",
    "        image_tensor = self.load_image_from_url(image_url)\n",
    "        \n",
    "        # Tokenize text\n",
    "        title_encoding = self.tokenizer(\n",
    "            title,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        desc_encoding = self.tokenizer(\n",
    "            description,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get target price\n",
    "        price = float(self.df.iloc[idx]['currentPrice'])\n",
    "        \n",
    "        return {\n",
    "            'title_input_ids': title_encoding['input_ids'].squeeze(),\n",
    "            'title_attention_mask': title_encoding['attention_mask'].squeeze(),\n",
    "            'desc_input_ids': desc_encoding['input_ids'].squeeze(),\n",
    "            'desc_attention_mask': desc_encoding['attention_mask'].squeeze(),\n",
    "            'image': image_tensor,\n",
    "            'price': torch.tensor(price, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4de5a1-3a15-4e27-b4b3-8ed8f7a74cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PricePredictor(nn.Module):\n",
    "    def __init__(self, transformer_model, image_encoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text encoder (BERT)\n",
    "        self.transformer = transformer_model\n",
    "        self.transformer_dim = self.transformer.config.hidden_size\n",
    "        \n",
    "        # Image encoder (ResNet50)\n",
    "        self.image_encoder = image_encoder\n",
    "        self.image_dim = 2048  # ResNet50 output dimension\n",
    "        \n",
    "        # Text fusion layer\n",
    "        self.text_fusion = nn.Sequential(\n",
    "            nn.Linear(self.transformer_dim * 2, self.transformer_dim),\n",
    "            nn.LayerNorm(self.transformer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Multimodal fusion layer\n",
    "        self.multimodal_fusion = nn.Sequential(\n",
    "            nn.Linear(self.transformer_dim + self.image_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Price prediction layers\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Final activation for positive prices\n",
    "        self.final_activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, title_input_ids, title_attention_mask, \n",
    "                desc_input_ids, desc_attention_mask, images):\n",
    "        # Process text\n",
    "        title_output = self.transformer(\n",
    "            input_ids=title_input_ids,\n",
    "            attention_mask=title_attention_mask\n",
    "        )\n",
    "        \n",
    "        desc_output = self.transformer(\n",
    "            input_ids=desc_input_ids,\n",
    "            attention_mask=desc_attention_mask\n",
    "        )\n",
    "        \n",
    "        # Get [CLS] token outputs\n",
    "        title_features = title_output.last_hidden_state[:, 0, :]\n",
    "        desc_features = desc_output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Fuse text features\n",
    "        text_combined = torch.cat([title_features, desc_features], dim=1)\n",
    "        text_fused = self.text_fusion(text_combined)\n",
    "        \n",
    "        # Process image\n",
    "        image_features = self.image_encoder(images)\n",
    "        image_features = image_features.view(image_features.size(0), -1)\n",
    "        \n",
    "        # Combine text and image features\n",
    "        multimodal_features = torch.cat([text_fused, image_features], dim=1)\n",
    "        fused_features = self.multimodal_fusion(multimodal_features)\n",
    "        \n",
    "        # Final prediction\n",
    "        output = self.regressor(fused_features)\n",
    "        return self.final_activation(output).squeeze()\n",
    "        \n",
    "def count_parameters_by_layer(model):\n",
    "   print(\"\\nParameters by Layer:\")\n",
    "   print(\"-\" * 50)\n",
    "   \n",
    "   # BERT layers\n",
    "   bert_params = sum(p.numel() for p in model.transformer.parameters())\n",
    "   print(f\"1. BERT Encoder: {bert_params:,} parameters\")\n",
    "   \n",
    "   # ResNet layers\n",
    "   resnet_params = sum(p.numel() for p in model.image_encoder.parameters())\n",
    "   print(f\"2. ResNet Encoder: {resnet_params:,} parameters\")\n",
    "   \n",
    "   # Text Fusion layers\n",
    "   print(\"\\n3. Text Fusion Layer:\")\n",
    "   for i, layer in enumerate(model.text_fusion):\n",
    "       layer_params = sum(p.numel() for p in layer.parameters())\n",
    "       print(f\"   {i+1}. {layer.__class__.__name__}: {layer_params:,} parameters\")\n",
    "   \n",
    "   # Multimodal Fusion layers\n",
    "   print(\"\\n4. Multimodal Fusion Layer:\")\n",
    "   for i, layer in enumerate(model.multimodal_fusion):\n",
    "       layer_params = sum(p.numel() for p in layer.parameters())\n",
    "       print(f\"   {i+1}. {layer.__class__.__name__}: {layer_params:,} parameters\")\n",
    "   \n",
    "   # Regressor layers\n",
    "   print(\"\\n5. Regressor Layers:\")\n",
    "   for i, layer in enumerate(model.regressor):\n",
    "       layer_params = sum(p.numel() for p in layer.parameters())\n",
    "       print(f\"   {i+1}. {layer.__class__.__name__}: {layer_params:,} parameters\")\n",
    "   \n",
    "   # Total parameters\n",
    "   total_params = sum(p.numel() for p in model.parameters())\n",
    "   print(\"\\n\" + \"-\" * 50)\n",
    "   print(f\"Total Parameters: {total_params:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe9076a-6061-4cbd-b93e-8d4ec3944987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sise/eliorsu-group/yuvalgor/courses/Data-mining-in-Big-Data/src/preprocessing.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['title'] = df['title'].apply(lambda x: str(x).lower().strip() if pd.notna(x) else '')\n",
      "/sise/eliorsu-group/yuvalgor/courses/Data-mining-in-Big-Data/src/preprocessing.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['description'] = df['description'].apply(lambda x: str(x).lower().strip() if pd.notna(x) else '')\n",
      "/sise/eliorsu-group/yuvalgor/courses/Data-mining-in-Big-Data/src/preprocessing.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['state_encoded'] = le_state.transform(df['pickupState'])\n",
      "/sise/eliorsu-group/yuvalgor/courses/Data-mining-in-Big-Data/src/preprocessing.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['category_encoded'] = le_category.transform(df['mainCategory'])\n"
     ]
    }
   ],
   "source": [
    "# Parameters for loading data\n",
    "directory = \"/sise/eliorsu-group/yuvalgor/courses/Data-mining-in-Big-Data/datasets/\"\n",
    "base_filename = \"goodwill_items_job_\"\n",
    "num_files = 30\n",
    "\n",
    "# Load and combine the CSV files\n",
    "combined_df = load_and_combine_csv_files(directory, base_filename, num_files)\n",
    "\n",
    "# Clean and label the data\n",
    "cleaned_df, le_state, le_category = clean_and_label_data(combined_df)\n",
    "\n",
    "# Split data using first 400000 rows for training\n",
    "train_val_df = cleaned_df.iloc[:400000].copy()\n",
    "test_df = cleaned_df.iloc[400000:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f10a19fe-a87e-4a6b-b030-0be82a8040bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvalgor/.conda/envs/goodwill_proj/lib/python3.9/site-packages/huggingface_hub-0.27.0-py3.8.egg/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters by Layer:\n",
      "--------------------------------------------------\n",
      "1. BERT Encoder: 109,482,240 parameters\n",
      "2. ResNet Encoder: 23,508,032 parameters\n",
      "\n",
      "3. Text Fusion Layer:\n",
      "   1. Linear: 1,180,416 parameters\n",
      "   2. LayerNorm: 1,536 parameters\n",
      "   3. ReLU: 0 parameters\n",
      "   4. Dropout: 0 parameters\n",
      "\n",
      "4. Multimodal Fusion Layer:\n",
      "   1. Linear: 1,442,304 parameters\n",
      "   2. LayerNorm: 1,024 parameters\n",
      "   3. ReLU: 0 parameters\n",
      "   4. Dropout: 0 parameters\n",
      "\n",
      "5. Regressor Layers:\n",
      "   1. Linear: 131,328 parameters\n",
      "   2. LayerNorm: 512 parameters\n",
      "   3. ReLU: 0 parameters\n",
      "   4. Dropout: 0 parameters\n",
      "   5. Linear: 32,896 parameters\n",
      "   6. LayerNorm: 256 parameters\n",
      "   7. ReLU: 0 parameters\n",
      "   8. Dropout: 0 parameters\n",
      "   9. Linear: 129 parameters\n",
      "\n",
      "--------------------------------------------------\n",
      "Total Parameters: 135,780,673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvalgor/.conda/envs/goodwill_proj/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize text encoder (BERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "transformer_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize image encoder (ResNet50)\n",
    "image_model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "# Remove the final classification layer\n",
    "image_model = nn.Sequential(*list(image_model.children())[:-1])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = GoodwillDataset(train_val_df, tokenizer)\n",
    "test_dataset = GoodwillDataset(test_df, tokenizer)\n",
    "\n",
    "# Create dataloaders with multiple workers for faster loading\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True,\n",
    "    num_workers=4,  # Adjust based on your CPU cores\n",
    "    pin_memory=True  # Faster data transfer to GPU\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize model with both encoders\n",
    "model = PricePredictor(\n",
    "    transformer_model=transformer_model,\n",
    "    image_encoder=image_model\n",
    ").to(device)\n",
    "\n",
    "count_parameters_by_layer(model)\n",
    "# Freeze the pre-trained models (optional)\n",
    "# for param in transformer_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in image_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer with different learning rates for pre-trained and new layers\n",
    "# Collect parameters that require gradients\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() if 'transformer' not in n and 'image_encoder' not in n],\n",
    "        'lr': 1e-3  # Higher learning rate for new layers\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() if 'transformer' in n or 'image_encoder' in n],\n",
    "        'lr': 1e-5  # Lower learning rate for pre-trained layers\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    weight_decay=0.01  # L2 regularization\n",
    ")\n",
    "\n",
    "# Learning rate scheduler (optional)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed0755-c219-4676-b874-b14b9c6b4b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 23584.2382:   1%|          | 226/26179 [36:19<61:56:37,  8.59s/it] "
     ]
    }
   ],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader)\n",
    "    for batch in progress_bar:\n",
    "        # Move all inputs to device\n",
    "\n",
    "        title_input_ids = batch['title_input_ids'].to(device)\n",
    "        title_attention_mask = batch['title_attention_mask'].to(device)\n",
    "        desc_input_ids = batch['desc_input_ids'].to(device)\n",
    "        desc_attention_mask = batch['desc_attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        price = batch['price'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(title_input_ids, title_attention_mask,\n",
    "                      desc_input_ids, desc_attention_mask,\n",
    "                      images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, price)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update total loss and progress bar\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (progress_bar.n + 1)\n",
    "        progress_bar.set_description(f'Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "num_epochs = 2\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    avg_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab71f82-f905-4924-900c-713321100a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "   model.eval()\n",
    "   predictions = []\n",
    "   actuals = []\n",
    "   titles = []\n",
    "   descriptions = []\n",
    "   \n",
    "   with torch.no_grad():\n",
    "       for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "           # Move batch to device\n",
    "           title_input_ids = batch['title_input_ids'].to(device)\n",
    "           title_attention_mask = batch['title_attention_mask'].to(device)\n",
    "           desc_input_ids = batch['desc_input_ids'].to(device)\n",
    "           desc_attention_mask = batch['desc_attention_mask'].to(device)\n",
    "           images = batch['image'].to(device)\n",
    "           price = batch['price']\n",
    "           \n",
    "           # Get predictions\n",
    "           output = model(title_input_ids, title_attention_mask,\n",
    "                        desc_input_ids, desc_attention_mask,\n",
    "                        images)\n",
    "           \n",
    "           # Store predictions and actual values\n",
    "           predictions.extend(output.cpu().numpy())\n",
    "           actuals.extend(price.numpy())\n",
    "           \n",
    "           # Store original text\n",
    "           titles.extend(tokenizer.batch_decode(title_input_ids, skip_special_tokens=True))\n",
    "           descriptions.extend(tokenizer.batch_decode(desc_input_ids, skip_special_tokens=True))\n",
    "   \n",
    "   return np.array(predictions), np.array(actuals), titles, descriptions\n",
    "\n",
    "# Save model with evaluation metrics\n",
    "\n",
    "\n",
    "# Get predictions\n",
    "predictions, actuals, titles, descriptions = evaluate(model, test_loader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb0130-ae82-40e4-93c2-aa70a7443fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'ImageTextFusion'\n",
    "save_dir = '/sise/eliorsu-group/yuvalgor/courses/Data-mining-in-Big-Data/models'\n",
    "if not os.path.exists(save_dir):\n",
    "   os.makedirs(save_dir)\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(actuals, predictions)\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\") \n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(save_dir, f'{model_name}.pth')\n",
    "torch.save({\n",
    "   'model_state_dict': model.state_dict(),\n",
    "   'optimizer_state_dict': optimizer.state_dict(),\n",
    "   'test_mse': mse,\n",
    "   'test_rmse': rmse,\n",
    "   'test_r2': r2,\n",
    "   'tokenizer': tokenizer,\n",
    "}, model_path)\n",
    "print(f'\\nModel saved at: {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25965f4b-2a2b-4a0e-88f2-b7d57494f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Add predictions and calculate metrics\n",
    "test_df['predicted_price'] = predictions\n",
    "test_df['actual_price'] = test_df['currentPrice']\n",
    "test_df['price_difference'] = test_df['predicted_price'] - test_df['actual_price']\n",
    "test_df['price_difference_pct'] = ((test_df['actual_price'] - test_df['predicted_price']) / test_df['predicted_price']) * -100\n",
    "\n",
    "# Create analysis dataframe with all relevant fields\n",
    "analysis_df = test_df[[\n",
    "   'title',\n",
    "   'actual_price', \n",
    "   'predicted_price',\n",
    "   'price_difference',\n",
    "   'price_difference_pct',\n",
    "   'mainCategory',\n",
    "   'description',\n",
    "   'pickupState',\n",
    "   'imageUrls',\n",
    "   'itemId'\n",
    "]].copy()\n",
    "\n",
    "# Round numeric columns\n",
    "numeric_cols = ['actual_price', 'predicted_price', 'price_difference', 'price_difference_pct']\n",
    "analysis_df[numeric_cols] = analysis_df[numeric_cols].round(2)\n",
    "\n",
    "# Sort by price difference (descending order - largest gap first)\n",
    "analysis_df = analysis_df.sort_values('price_difference', ascending=False)\n",
    "\n",
    "# Save results\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "model_name = 'ImageTextFusion'\n",
    "filename = f\"{model_name}_predictions_{timestamp}.csv\"\n",
    "save_path = f\"/sise/eliorsu-group/yuvalgor/courses/Data-mining-in-Big-Data/results/{filename}\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "# Save analysis\n",
    "analysis_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"\\nAnalysis results saved at: {save_path}\")\n",
    "print(f\"Total predictions: {len(analysis_df)}\")\n",
    "\n",
    "# Display sample of predictions\n",
    "print('\\nSample of predictions:')\n",
    "sample_cols = ['title', 'actual_price', 'predicted_price', 'price_difference_pct']\n",
    "print(analysis_df[sample_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a71b43-ab4f-4249-bba5-f529a73401c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b5e6f0-9df0-474e-81b2-a0061441ecb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (goodwill_proj)",
   "language": "python",
   "name": "goodwill_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
